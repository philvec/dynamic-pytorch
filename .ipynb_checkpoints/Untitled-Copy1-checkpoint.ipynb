{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'iris.csv'\n",
    "dt = np.genfromtxt('data/' + dataset_name, delimiter=',', skip_header=1)\n",
    "xs, ys = dt[:, :-1], dt[:, -1].astype(int)\n",
    "xs = (xs - np.mean(xs, axis=0)) / np.std(xs, axis=0)\n",
    "data = list(zip([torch.from_numpy(x).float() for x in xs], [torch.tensor([y]).long() for y in ys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InspLinear(torch.nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super(InspLinear, self).__init__(in_features, out_features, bias)\n",
    "        \n",
    "        self.clear_acc()\n",
    "        self.dropout = torch.ones_like(self.weight)\n",
    "        \n",
    "    def update_acc(self):\n",
    "        self.w_grad_acc.append(self.weight.grad.detach().clone())\n",
    "        self.b_grad_acc.append(self.bias.grad.detach().clone())\n",
    "        \n",
    "    def clear_acc(self):\n",
    "        self.w_grad_acc= []\n",
    "        self.b_grad_acc = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.dropout*self.weight, self.bias)\n",
    "        \n",
    "    def get_acc(self):\n",
    "        return self.w_grad_acc, self.b_grad_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(dataset, model_=None, forcing=False):\n",
    "    model = copy.deepcopy(model_)\n",
    "    layer = 2\n",
    "    #model.to('cuda')\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    results = []\n",
    "    for epoch in range(100):\n",
    "        fr = forcing and epoch%3 == 0\n",
    "        if fr:\n",
    "            # testing gradients\n",
    "            model.train()\n",
    "            model[layer].clear_acc()\n",
    "\n",
    "            for i, (x, y) in enumerate(dataset):\n",
    "                prediction = model(x.unsqueeze(dim=0))\n",
    "                train_loss = criterion(prediction, y)\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                model[layer].update_acc()\n",
    "                \n",
    "            w_acc, b_acc = model[layer].get_acc()\n",
    "            w_mean = torch.mean(torch.stack(w_acc), dim=0)\n",
    "            b_mean = torch.mean(torch.stack(b_acc), dim=0)\n",
    "\n",
    "            std = torch.sum(torch.std(torch.stack(w_acc), dim=0), dim=-1) + torch.std(torch.stack(b_acc), dim=0)\n",
    "            chosen = torch.argsort(std)[-2:]\n",
    "            #print(std, chosen)\n",
    "\n",
    "            mask = []\n",
    "            for i, (x, y) in enumerate(dataset):\n",
    "                prediction = model(x.unsqueeze(dim=0))\n",
    "                train_loss = criterion(prediction, y)\n",
    "                optimizer.zero_grad()\n",
    "                train_loss.backward()\n",
    "                mask.append(1 if torch.sum(model[0].weight.grad[chosen[0].item()]) > torch.sum(w_mean[chosen[0].item()]) else 0)\n",
    "            splits = [0, 1]\n",
    "        else:\n",
    "            mask = [0]*len(dataset)\n",
    "            splits=[0]\n",
    "                \n",
    "        # training\n",
    "        random.shuffle(dataset)\n",
    "        model.train()\n",
    "        acc = 0.\n",
    "        for split in splits:\n",
    "            for i, (x, y) in enumerate(dataset):\n",
    "                if fr:\n",
    "                    model[layer].dropout = chosen[split]*torch.ones_like(model[layer].weight)\n",
    "                    model[layer].dropout[chosen[split]] = abs(chosen[split].item() - 1.)\n",
    "                else:\n",
    "                    model[layer].dropout = torch.ones_like(model[layer].weight)\n",
    "                if mask[i] == split:\n",
    "                    prediction = model(x.unsqueeze(dim=0))\n",
    "                    train_loss = criterion(prediction, y)\n",
    "                    optimizer.zero_grad()\n",
    "                    train_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #losses.append(train_loss.item())\n",
    "                    if torch.argmax(prediction.squeeze()) == y:\n",
    "                        acc += 1.\n",
    "        acc /= len(dataset)\n",
    "        results.append(acc)\n",
    "        if acc == 1.:\n",
    "            break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fe5d1a6ec843a895ab65eb42c8d418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "winners = []\n",
    "winners_v = []\n",
    "winners_a=[]\n",
    "winners_b=[]\n",
    "dataset = data\n",
    "for i in trange(100):\n",
    "    model_ = torch.nn.Sequential(InspLinear(4, 10), torch.nn.Sigmoid(), \n",
    "                                 InspLinear(10, 10), torch.nn.Sigmoid(), \n",
    "                                 InspLinear(10, 3))\n",
    "    epochs = []\n",
    "    for forcing in [False, True]:\n",
    "        res = train(dataset, model_, forcing)\n",
    "        #plt.plot(range(1, len(res)+1), res)\n",
    "        #print('Forcing:', forcing, np.argmax(res)+1, np.max(res))\n",
    "        epochs.append(res)\n",
    "        \n",
    "    winners.append(-1. if np.argmax(epochs[0]) < np.argmax(epochs[1]) else (1. if np.argmax(epochs[0]) > np.argmax(epochs[1]) else 0.))\n",
    "    winners_v.append(-1. if np.max(epochs[0]) > np.max(epochs[1]) else (1. if np.max(epochs[0]) < np.max(epochs[1]) else 0.))\n",
    "    \n",
    "    winners_a.append(1. if winners[-1]>0. or winners_v[-1]>0. else 0.)\n",
    "    winners_b.append(1. if winners[-1]<0. or winners_v[-1]<0. else 0.)\n",
    "    #plt.legend(['Standard', 'With grad forcing'])\n",
    "    #plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08\n",
      "(array([-1.,  0.,  1.]), array([44,  4, 52], dtype=int64))\n",
      "0.66\n",
      "(array([-1.,  0.,  1.]), array([ 5, 24, 71], dtype=int64))\n",
      "0.91\n",
      "0.48\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(winners))\n",
    "print(np.unique(winners, return_counts=True))\n",
    "print(np.mean(winners_v))\n",
    "print(np.unique(winners_v, return_counts=True))\n",
    "print(np.mean(winners_a))\n",
    "print(np.mean(winners_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
